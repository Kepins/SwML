{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de0ef89-f812-40eb-9772-de334e634696",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a67dfa-2a9b-4594-a298-80282258669a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 23:28:09.547919: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 23:28:09.579038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 23:28:10.095170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfc681-f694-4328-ae89-8aa39546b629",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5456abc-c933-4b83-8b05-2267bf0f5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = Path(\"original\")\n",
    "preprocesed_dataset_path = Path(\"preprocessed\")\n",
    "augmented_dataset_path = Path(\"augmented\")\n",
    "\n",
    "dims = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eae714-34c0-425f-b3f2-3ab0695d2f25",
   "metadata": {},
   "source": [
    "## Create preprocesed_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69a32e0-9563-439a-854c-facc6ae90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesed_dataset_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c2796-ed7a-42ab-b742-a1bcf4a5aa53",
   "metadata": {},
   "source": [
    "## Resize funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd5dc18-4feb-4161-8bc3-3b6491e97654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_center_box(org_img, dims):\n",
    "    org_height, org_width, _ = org_img.shape\n",
    "    min_org_dims = min(org_height, org_width)\n",
    "    if org_height == min_org_dims:\n",
    "        pixels_skip = (org_width-min_org_dims)//2\n",
    "        org_roi = org_img[0:org_height, pixels_skip: org_width-pixels_skip]\n",
    "    else:\n",
    "        pixels_skip = (org_height-min_org_dims)//2\n",
    "        org_roi = org_img[pixels_skip:org_height-pixels_skip, 0: org_width]\n",
    "    return cv2.resize(org_roi, dims)\n",
    "\n",
    "def resize_strech(org_img, dims):\n",
    "    return cv2.resize(org_img, dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24de46e-5be4-4aeb-9798-b1bfcd807714",
   "metadata": {},
   "source": [
    "## Load classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a165af-0c47-464f-9edc-5d5ac5be7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_to_klass = {animal: klass for klass, animal in enumerate(os.listdir(original_dataset_path))}\n",
    "klass_to_animal = {klass: animal for animal, klass in animal_to_klass.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2faf179-eda2-4fad-b6a4-0a42242fc5f3",
   "metadata": {},
   "source": [
    "## Original class dirstibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e3a8ea-3b4f-4d85-8c1d-1a02729d964c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4821, 1: 1820, 2: 3098, 3: 2623, 4: 4319}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klass_initial_count = {\n",
    "    klass: len(os.listdir(os.path.join(original_dataset_path, animal))) for klass, animal in klass_to_animal.items()\n",
    "}\n",
    "klass_initial_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507ebfc-e201-4e7b-8a2f-a0107aacebe5",
   "metadata": {},
   "source": [
    "## Make each img have provided dims and create df_dataset_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67fceeae-7833-405b-8ff2-c599057e14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = {\n",
    "    \"klass\": [],\n",
    "    \"img_path\": [],\n",
    "}\n",
    "\n",
    "for animal in animal_to_klass.keys():\n",
    "    original_klass_path = original_dataset_path.joinpath(animal)\n",
    "    preprocessed_klass_path = preprocesed_dataset_path.joinpath(animal)\n",
    "    \n",
    "    # Create dir for class if not exists\n",
    "    preprocessed_klass_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx, file in enumerate(os.listdir(original_klass_path)):\n",
    "        original_file_path = original_klass_path.joinpath(file)\n",
    "        preprocessed_file_path = preprocessed_klass_path.joinpath(f\"{idx}.jpeg\")\n",
    "\n",
    "        # Read img\n",
    "        org_img = cv2.imread(str(original_file_path))\n",
    "        \n",
    "        # Provide resize_center_box() or resize_strech()\n",
    "        resized_img = resize_center_box(org_img, dims)\n",
    "\n",
    "        # Save img\n",
    "        cv2.imwrite(str(preprocessed_file_path), resized_img)\n",
    "\n",
    "        dataset_meta[\"klass\"].append(animal_to_klass[animal])\n",
    "        dataset_meta[\"img_path\"].append(str(preprocessed_file_path))\n",
    "\n",
    "df_dataset_meta = pd.DataFrame(dataset_meta[\"klass\"], index=dataset_meta[\"img_path\"], columns=[\"klass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e6745d-42a8-4fe5-bbf3-acb19647b763",
   "metadata": {},
   "source": [
    "## 9.1 Split original into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c639c5-e244-4b39-9495-461d39227c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 43  # Seed value for reproducibility\n",
    "\n",
    "val_size = 250\n",
    "test_size = 250\n",
    "train_size = df_dataset_meta.shape[0] - val_size - test_size\n",
    "\n",
    "df_train_meta = df_dataset_meta.sample(train_size, random_state=seed_value)\n",
    "df_remainging_meta = df_dataset_meta.drop(df_train_meta.index)\n",
    "df_val_meta = df_remainging_meta.sample(val_size, random_state=seed_value)\n",
    "df_test_meta = df_remainging_meta.drop(df_val_meta.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebff2d-4bcf-4932-9adf-860c03ba6db1",
   "metadata": {},
   "source": [
    "## 9.2 Uniform class distribution with normalized and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1809ca9-46b1-4d61-9eae-d45ae9b3f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_to_count = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b92266-e858-4c89-84ac-e37a6b111b7a",
   "metadata": {},
   "source": [
    "### Distribution after val/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087b27b6-c40e-4a0b-988a-88a364435128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4672, 4: 4194, 2: 3003, 3: 2543, 1: 1769}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klass_train_count = df_train_meta[\"klass\"].value_counts().to_dict()\n",
    "klass_train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21480d8b-25f9-48e4-be9c-035cc4b0cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_augmnented_meta = {\n",
    "    \"klass\": [],\n",
    "    \"img_path\": [],\n",
    "}\n",
    "\n",
    "# Define augmentation pipeline\n",
    "seq = iaa.Sequential([\n",
    "    # iaa.Affine(scale=(1, 1.1)), # Scale from 1 to 1.1\n",
    "    iaa.Fliplr(0.5),  # horizontally flip 50% of the images\n",
    "    iaa.Affine(rotate=(-15, 15)),  # rotate images by -15 to 15 degrees\n",
    "    iaa.Resize({\"height\": dims[1], \"width\": dims[0]}),\n",
    "    iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),\n",
    "    iaa.Crop(px=(5, 15)),  # crop images from each side by 0 to 20px (randomly chosen)\n",
    "])\n",
    "\n",
    "avg_img_augment_count = {}\n",
    "how_many_should_be_augmented = {}\n",
    "augmented = {}\n",
    "\n",
    "for animal in animal_to_klass.keys():\n",
    "    augmentend_klass_path = augmented_dataset_path.joinpath(animal)\n",
    "    \n",
    "    # Create dir for class if not exists\n",
    "    augmentend_klass_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    avg_img_augment_count[animal] = augment_to_count / klass_train_count[animal_to_klass[animal]] - 1\n",
    "    how_many_should_be_augmented[animal] = 0\n",
    "    \n",
    "    augmented[animal]=0\n",
    "\n",
    "\n",
    "for idx, (preprocessed_file_path, row)  in enumerate(df_train_meta.iterrows()):\n",
    "    klass = row[\"klass\"]\n",
    "\n",
    "    animal = klass_to_animal[klass]\n",
    "\n",
    "    preprocessed_klass_path = preprocesed_dataset_path.joinpath(animal)\n",
    "    augmentend_klass_path = augmented_dataset_path.joinpath(animal)\n",
    "    \n",
    "    how_many_should_be_augmented[animal] += avg_img_augment_count[animal]\n",
    "\n",
    "    current_augment_count = int(how_many_should_be_augmented[animal]) #  this many augmentations of this img need to be added\n",
    "    how_many_should_be_augmented[animal] -= current_augment_count\n",
    "\n",
    "    \n",
    "    # Read img\n",
    "    org_img = cv2.imread(str(preprocessed_file_path))\n",
    "\n",
    "    dataset_augmnented_meta[\"klass\"].append(animal_to_klass[animal])\n",
    "    dataset_augmnented_meta[\"img_path\"].append(str(preprocessed_file_path))\n",
    "    \n",
    "    # Augment the image\n",
    "    augmented_images = seq(images=[org_img] * current_augment_count)\n",
    "\n",
    "    # Save augmented images\n",
    "    for augmented_image in augmented_images:\n",
    "        augmented_file_path = augmentend_klass_path.joinpath(f\"{augmented[animal]}.jpeg\")\n",
    "        cv2.imwrite(str(augmented_file_path), augmented_image)\n",
    "        dataset_augmnented_meta[\"klass\"].append(animal_to_klass[animal])\n",
    "        dataset_augmnented_meta[\"img_path\"].append(str(augmented_file_path))\n",
    "        augmented[animal] += 1\n",
    "\n",
    "df_dataset_augmented_meta = pd.DataFrame(dataset_augmnented_meta[\"klass\"], index=dataset_augmnented_meta[\"img_path\"], columns=[\"klass\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e74cd967-a715-45af-857c-1a9c13a3ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_augmented_meta = df_dataset_augmented_meta.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e2bcf-51be-42d1-913e-48a3dbe6a37e",
   "metadata": {},
   "source": [
    "## 9.3 Val is part of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff1f766-8810-45f8-8758-5b599b0f8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 43  # Seed value for reproducibility\n",
    "\n",
    "df_train_augmented_plus_val_meta = pd.concat([df_dataset_augmented_meta, df_val_meta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc8f5a-fda7-4150-9f03-003d8cf5a78e",
   "metadata": {},
   "source": [
    "## Load images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffcc7327-74ce-4906-bfe0-92322d2bd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_dir = Path(\"SPLIT_1\")\n",
    "split2_dir = Path(\"SPLIT_2\")\n",
    "split3_dir = Path(\"SPLIT_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163b2f49-5c6e-45f2-a86f-cd0f28c7482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_dir(dir, df):\n",
    "    class_to_dir = {}\n",
    "    class_counter = {}\n",
    "    for animal in animal_to_klass.keys():\n",
    "        animal_dir = dir.joinpath(Path(animal))\n",
    "        animal_dir.mkdir(parents=True, exist_ok=True)\n",
    "        class_to_dir[animal_to_klass[animal]] = animal_dir\n",
    "        class_counter[animal_to_klass[animal]] = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        klass = row[\"klass\"]\n",
    "        org_filepath = Path(idx)\n",
    "        new_filepath = Path(class_to_dir[row[\"klass\"]].joinpath(f\"{str(class_counter[klass])}.jpeg\"))\n",
    "        class_counter[klass] += 1\n",
    "        shutil.copyfile(org_filepath, new_filepath)\n",
    "        \n",
    "\n",
    "def save_splits(split_dir, df_train_meta, df_val_meta, df_test_meta):\n",
    "    train_dir = split_dir.joinpath(Path(\"training\"))\n",
    "    val_dir = split_dir.joinpath(Path(\"validation\"))\n",
    "    test_dir = split_dir.joinpath(Path(\"testing\"))\n",
    "\n",
    "    # Create dirs\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    val_dir.mkdir(parents=True, exist_ok=True)\n",
    "    test_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    save_df_to_dir(train_dir, df_train_meta)\n",
    "    save_df_to_dir(val_dir, df_val_meta)\n",
    "    save_df_to_dir(test_dir, df_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64350a3a-4361-452f-a413-bc93cf7bd6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_splits(split1_dir, df_train_meta, df_val_meta, df_test_meta)\n",
    "save_splits(split2_dir, df_train_augmented_meta, df_val_meta, df_test_meta)\n",
    "save_splits(split3_dir, df_train_augmented_plus_val_meta, df_val_meta, df_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65596be3-82a1-4563-b383-de42e3b7974d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
